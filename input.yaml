# Input file

project:
  name: "Linear-Advection_1D1d"
  status: "sota" 
  info-techniques: 
    Non-Dimensionalization: true
    Adaptive sampling with in the Mesh: true
    Exponential decay: true 
    Modified Deeponet: true
    Loss Balancing: true (data_guided_weights)
    Modified MLP: false 
   

dataset:
  path: '.dataset'
  generate: true # true or false
  processing: true  # true or false
  train_fraction: 0.68
  # val_fraction: 0.02
  test_fraction: 0.3 
  varying_param: [0.5,0.6,0.7,0.8,0.9,1.0,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2.0,0.56,1.25,1.95] # parameter c
  varying_param_label: ['C05','C06','C07','C08','C09','C10','C11','C12','C13','C14','C15','C16','C17','C18','C19','C20','C056','C125','C195'] # folder_ID
  strata_labels: ['PHY','BC']
  state_vector_labels: ['coord_x','u'] 
  N_samples_per_stratum: {'N_coll': 10000}
  L: 1.0
  c: 1.0
  initial_time: 0.0
  N_time_step: 51
  time_step: 0.02
  total_time: 1
 

pde_param:
  name: 'linear_advection'
  k: {}

branch1:
  neuralNet:
    architecture : "Modified Deeponet"  # "Mlp" or "Modified MLP" or "Modified Deeponet"
    in_dim : 1  # 'm_input_sensors' 
    num_layers : 3
    hidden_dim : 50
    out_dim : 128 
    activation : "tanh" # "swish", "tanh", "gelu"
    xavier_init : true

branches_control:
  branch_list_ID: ['branch1']
  branch_input_ID: ['k'] 
  

trunk:
  neuralNet:
    architecture : "Modified Deeponet"  # "Mlp" or "Modified MLP" or "Modified Deeponet"
    in_dim : 2
    num_layers : 3
    hidden_dim : 50
    out_dim : 128 
    activation : "tanh" # "swish", "tanh", "gelu"
    xavier_init : true

  
train: 
  n_training_param: 16
  training_param: [0.5,0.6,0.7,0.8,0.9,1.0,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2.0] # parameter k
  training_param_label: ['C05','C06','C07','C08','C09','C10','C11','C12','C13','C14','C15','C16','C17','C18','C19','C20']
  adam_steps: 50000
  lbfgs_steps: 0
  stop_iter: -1 
  batch_size_coll: 1024
  batch_dfraction:  # Specific decimal fraction with respect to total data in boundary conditions {Inlet, Wall, Outlet}, initial condition and validation ds
    data: 1.0
    ic: 1.0
    bc: 1.0
    val: 1.0
  data_extraction: 
     mode: 'random' # 'none', 'random'
     decimal_fraction: 0.01 # mode: 'none', 'random'

test: 
  n_test_param: 3
  test_param: [0.56,1.25,1.95] # parameter k
  test_param_label: ['C056','C125','C195']
  batch_size: 10000
  t_values: [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]
 

optim1:
  optimizer : 'Adam'
  beta1: 0.9
  beta2: 0.999
  eps: 1.0e-8
  learning_rate: 1.0e-3
  exponential_decay:
    enabled: true
    decay_rate: 0.95  
    decay_steps: 3000

optim2:
  optimizer : 'LBFGS'
  max_iter: 50 
  max_eval: 50
  history_size: 50
  learning_rate : 1.0

tl: # transfer learning
  stop_total_loss: -1


fourier_emb:  # Fourier Features Embedding
  enabled: false
  stddev: 1.0
  embed_dim: 128


random_weight_fact:  # Random Weight Factorization
  enabled: false
  mean : 0.5 
  stddev : 0.1


loss_terms: ["phy","bc", "ic", "data"]
lambda_weights_init: [1.0, 1.0, 1.0, 1.0]

loss_balancing:
  scheme: 'data_guided_weights'
  enabled: true
  update_step: 1000
  
  # scheme: 'no_weights'
  # enabled: true
  # update_step: 500
# lambda_weights_init: [1.0, 1.0, 1.0, 1.0]
  
#   scheme: 'fixed_weights'
#   enabled: true
#   update_step: 500
# lambda_weights_init: [1.0, 100.0, 100.0, 100.0]
  

  # scheme: 'ntk_guided_weights'
  # enabled: true
  # type: 'moderate_local_NTK_weights' # 'global_NTK_weights', 'local_NTK_weights', 'moderate_local_NTK_weights'
  # update_step: 500


# loss_balancing:  # Grad Norm - Self adaptive learning rate annealing algorithm 
  # enabled: false
  # scheme: "grad_norm"
  # momentum: 0.9
  # update_step: 1000
  

logging: 
  log_every_steps : 500
  monitoring_resources: true


seed: 42 
