###  Input file

project:
  name: "Linear-Advection_1D1d"
  status: "sota" 
  info-techniques: 
    Exponential decay: true 
    Grad Norm: false 
    Fourier embedding: false 
    Modified MLP: false 
    RWF: false

dataset:
  path: '.dataset'
  generate: false # true or false
  processing: false  # true or false
  train_fraction: 0.68
  # val_fraction: 0.02
  test_fraction: 0.3 
  varying_param: [0.5,0.875,1.25,1.625,2.0,0.56,0.67,0.93,1.12,1.5,1.73,1.95] # parameter c
  varying_param_label: ['C05','C0875','C125','C1625','C20','C056','C067','C093','C112','C15','C173','C195'] # folder_ID
  strata_labels: ['PHY']
  state_vector_labels: ['coord_x','u'] 
  N_samples_per_stratum: {'N_coll': 10000}
  initial_time: 0.0
  N_time_step: 51
  time_step: 0.02
  total_time: 1
 

pde_param:
  name: 'linear_advection'
  c: {}

branch1:
  neuralNet:
    architecture : "MLP"  # "Mlp" or "Modified MLP"
    in_dim : 90  # 'm_input_sensors' 
    num_layers : 3
    hidden_dim : 75
    out_dim : 50 
    activation : "tanh" # "swish", "tanh", "gelu"
    xavier_init : true

branches_control:
  branch_list_ID: ['branch1']
  branch_input_ID: ['c'] 
  

trunk:
  neuralNet:
    architecture : "MLP"  # "Mlp" or "Modified MLP"
    in_dim : 2
    num_layers : 3
    hidden_dim : 50
    out_dim : 50 
    activation : "tanh" # "swish", "tanh", "gelu"
    xavier_init : true

  
train: 
  n_training_param: 5
  training_param: [0.5,0.875,1.25,1.625,2.0] # parameter c
  training_param_label: ['C05','C0875','C125','C1625','C20']
  adam_steps: 50000
  lbfgs_steps: 0
  stop_iter: -1 
  batch_size_coll: 1024
  batch_dfraction:  # Specific decimal fraction with respect to total data in boundary conditions {Inlet, Wall, Outlet}, initial condition and validation ds
    data: 1.0
    ic: 1.0
    val: 1.0
  data_extraction: 
     mode: 'random' # 'none', 'random'
     decimal_fraction: 0.01 # mode: 'none', 'random'

test: 
  n_test_param: 12
  test_param: [0.5,0.875,1.25,1.625,2.0, 0.56,0.67,0.93,1.12,1.5,1.73,1.95] # parameter c
  test_param_label: ['C05','C0875','C125','C1625','C20', 'C056','C067','C093','C112','C15','C173','C195']
  batch_size: 10000
  t_values: [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]
 

optim1:
  optimizer : 'Adam'
  beta1: 0.9
  beta2: 0.999
  eps: 1.0e-8
  learning_rate: 1.0e-3
  exponential_decay:
    enabled: true
    decay_rate: 0.95  
    decay_steps: 3000

optim2:
  optimizer : 'LBFGS'
  max_iter: 50 
  max_eval: 50
  history_size: 50
  learning_rate : 1.0

tl: # transfer learning
  stop_total_loss: -1


fourier_emb:  # Fourier Features Embedding
  enabled: false
  stddev: 1.0
  embed_dim: 128


random_weight_fact:  # Random Weight Factorization
  enabled: false
  mean : 0.5 
  stddev : 0.1


loss_terms: ["phy", "ic", "data"]
lambda_weights_init: [1.0, 1.0, 1.0]

loss_balancing:  # Grad Norm - Self adaptive learning rate annealing algorithm 
  enabled: false
  scheme: "grad_norm"
  momentum: 0.9
  update_step: 1000

logging: 
  log_every_steps : 500
  monitoring_resources: false


seed: 42 
